#!/bin/sh
"export" "AWKPATH=$( dirname $( which piawka ) ):$AWKPATH"
"export" "AWKLIBPATH=$( realpath $( which gawk )/../../lib/gawk):$AWKLIBPATH"
"exec" "gawk" -f "$0" "--" "$@" && 0 {}

@load "fork"
@load "filefuncs"

@include "getopt.awk"
@include "argparse.awk"

BEGIN{ 
  OFS="\t"
  usage="piawka v0.8.0\nUsage:\npiawka -g groups_tsv -v vcf_gz [OPTIONS]"
  exit main()
}

function main() {
  # add_argument args: shortopt, longopt, is_flag, description
  argparse::add_argument("1", "persite", 1, "output values for each site")
  argparse::add_argument("a", "all", 1, "output more cols: numerator, denominator, nGeno, nMiss")
  argparse::add_argument("b", "bed", 0, "BED file with regions to be analyzed")
  argparse::add_argument("B", "targets", 0, "BED file with targets (faster than -b for numerous small regions)")
  argparse::add_argument("D", "nodxy", 1, "do not output Dxy (and other pairwise metrics)")
  argparse::add_argument("f", "fst", 1, "output Hudson Fst")
  argparse::add_argument("F", "fstwc", 1, "output Weir and Cockerham Fst instead")
  argparse::add_argument("g", "groups", 0, "2-columns sample ID / group ID TSV file")
  argparse::add_argument("h", "help", 1, "show this help message")
  argparse::add_argument("H", "het", 1, "output only per-sample pi = heterozygosity")
  argparse::add_argument("j", "jobs", 0, "number of parallel jobs to run")
  argparse::add_argument("m", "mult", 1, "use multiallelic sites")
  argparse::add_argument("M", "miss", 1, "max share of missing GT per group at site")
  argparse::add_argument("P", "nopi", 1, "do not output pi values")
  argparse::add_argument("r", "rho", 1, "output Ronfort's rho")
  argparse::add_argument("t", "tajimalike", 1, "output Tajima's D-like stat (manages missing data but isn't a test)")
  argparse::add_argument("T", "tajima", 1, "output vanilla Tajima's D instead (sensitive to missing data)")
  argparse::add_argument("v", "vcf", 0, "gzipped and tabixed VCF file")
  help = usage "\n" argparse::format_help()
  getopt::Optind = 1 # start with 1st opt
  getopt::Opterr = 1 # print getopt errs
  if ( argparse::parse_args() != 0 ) { print help; return 0 }
  for(i in argparse::args) { args[i]=argparse::args[i] } # shorten args array name
  if (args["help"] || ARGC==1 ) { print help; return 0 }

  assert( args["vcf"] != "", "required argument: -v <file.vcf.gz>" )
  assert( ( args["groups"] != "" || args["het"] == 1 ), "required argument: -g <groups.tsv>" )
  if ( args["bed"] != "" ) { check_file( args["bed"] ) }
  if ( args["targets"] != "" ) { check_file( args["targets"] ) }

  # Set default variable values
  if ( args["fst"] == 1 ) { FST = "HUD" }
  if ( args["fstwc"] == 1 ) { FST="WC" }
  if ( args["miss"] == "" ) { args["miss"]=1 }
  if ( args["mult"] == 1 && args["rho"] == 1 ) {
    print "Warning: rho was not meant for multiallelic sites" > "/dev/stderr"
  }
    
  VERBOSE=( args["all"] || ( args["jobs"]>1 && args["bed"]=="" ) )
  TAJLIKE=( args["tajima"] || args["tajimalike"] )

  get_groups()
  get_header()
  print_header()

  if ( args["jobs"] > 1 ) {
    if ( args["bed"] == "" ) {
      printf "" | "summarize_blks.awk" # initialize pipe
    }
    for ( jobnum=0; jobnum < args["jobs"]; jobnum++ ) { 
      pid=fork()
      if ( pid==0 ) { break }
    }
  } else { args["jobs"]=1; jobnum=1 }
  if ( !( args["bed"] == "" && args["targets"] == "" ) ) {
    while ( ( args["bed"]=="" ? ( "aggregate_regions.awk "args["targets"] | getline ) : ( getline < args["bed"] ) ) > 0 ) {
      bedline++
      if ( args["jobs"] > 1 && !( pid == 0 && ( bedline % args["jobs"] == jobnum ) ) ) { continue }
    if ( jobnum == 1 ) {
     print "" > "/dev/stderr" # to separate progress from header
     printf "\033[1Am\rProcessed "bedline" regions " > "/dev/stderr"
    }
      NSITES=$3-$2
      reg = $1":"$2+1"-"$3
      name = ( $4 == "" ? reg : $4 )
      process_sites(reg, name)
    }
    close( ( args["targets"] == "" ? args["bed"] : "aggregate_regions.awk "args["targets"] ) )
  } else {
    process_sites("", args["vcf"])
    close("summarize_blks.awk")
  } 
  return 0
}

# Groups file (first in command line): store lists of group members in `groups` array
function get_groups() { 
  check_file( args["groups"] )
  while ( getline < args["groups"] > 0 ) {
    if (!checked_groups) { assert(NF==2, "the groups file must contain two columns"); checked_groups=1 }
      groupmem[$1]= ( args["het"] == 1 ? $1 : $2 )
  }
  close( args["groups"] )
}

# VCF header: assign samples to groups
function get_header() {
  cmd="bgzip -dc " args["vcf"]
  while ( cmd | getline > 0 ) { 
    header_length++
    if ($0 ~ /^#CHROM/ ) {

      # Assign sample positions to groups
      for (i=10; i<=NF; i++) {
        if ( groupmem[$i] != "" ) { 
           groupindex[i]=groupmem[$i]
           groups[groupmem[$i]]++
         }
      }
      # Store group combinations
      if ( args["nodxy"] != 1 && args["persite"] != 1 ) {
        for (g in groups) {
          for (g2 in groups) {
            if (g2 < g) { combs[g,g2]++ }
          }
        }
      }
      # Assign ploidy to groups using variant calls from next line, detect mixed-ploidy groups
      if ( TAJLIKE || args["rho"] == 1 ) {
        cmd | getline
        # Count alleles for each group, then divide by number of samples
        for (i in groupindex) {
          ploidy[groupindex[i]] += ( match($i, ":")>0 ? match($i, ":") : length($i)+1 )
        }
        for (g in groups) {
          ploidy[g] /= 2*groups[g]
        }
        if ( ploidy[g] % 1 != 0 && ( TAJLIKE || args["rho"] == 1 ) ) {
          print "Warning: mixed ploidy population "g" will give unreliable results with " TAJLIKE ? "TAJLIKE" : "args["rho"] == 1" > "/dev/stderr"
        }
      }
      # Start processing sites if no need to query by regions?
      break
    }
  }
  close(cmd)
}

# Process VCF lines
function process_sites(region, locus,   
                       nUsed, nSites, minpos, maxpos, skip_header, vcfline,
                       numerator, denominator, 
                       allgeno, allmiss,
                       fst_numerator, fst_denominator,
                       segr, truesegr,
                       Hsnum, Hsden, Htnum, Htden, Hspnum) {
  if ( locus == "" ) { locus="_" } # empty locus breaks summarize_blks.awk
  cmd = ( region=="" ? "bgzip -dc " args["vcf"] : "tabix " args["vcf"] ( args["targets"] != "" ? " -T "args["targets"] : "" ) " " region )
  # Skip header in case of bgzip stream
  if ( region != "" ) { skip_header=header_length }
  while ( ++skip_header <= header_length ) { cmd | getline }
  while ( cmd | getline > 0 ) {
    vcfline++
    if ( args["jobs"] > 1 && region == "" && !( pid == 0 && ( vcfline % args["jobs"] == jobnum ) ) ) { continue }
    if ( jobnum == 1 && args["persite"] != 1 && region == "" && vcfline % 10000 < args["jobs"] ) {
     print "" > "/dev/stderr" # to separate progress from header
     printf "\033[1Am\rProcessed "vcfline" sites " > "/dev/stderr"
    }

    # Increment nSites
    if ( NSITES!="" ) { 
      nSites=NSITES
    } else if ( pid!="" ) { 
      nSites++ 
    } else {
      if ( !minpos[$1] ) { minpos[$1]=$2 }
      maxpos[$1]=$2
    }

    # Process only SNPs (possibly monomorphic or multiallelic)
    # To obtain results identical to ksamuk/pixy, set $5 !~ /\*|,|[ACGT][ACGT]/
    # To process SNPs that only have one ALT allele
    if ( $4 ~ /^[ACGT]$/ && $5 !~ /\*|[ACGT][ACGT]/ ) {
      #TODO: add check for chromosome not to summarize stats across chrs
    
      if ( args["persite"] == 1 ) { $1=locus"_"$1 }
    
      # Reset site-specific parameters
      for (g in groups) { miss[g]=0; misind[g]=0; nalleles[g]=0; nseen[g]=0 }
      delete alleles
      delete seen
      delete seenlist

      # Pool GT values for groups, count each state and missing data
      for (i in groupindex) {
        grp=groupindex[i]
        gtend=match( $i, ":" )
        if ( gtend==0 ) { gtend=length($i)+1 }
        for (c=1; c<gtend; c+=2) {
          al=substr($i,c,1)
          if ( al == "." ) {
            miss[grp]+=gtend/2
            break
          } else {
            if ( args["het"] == 1 ) { thisal[al]++ }
            alleles[grp,al]++
            nalleles[grp]++
            if ( !seen[grp,al] ) {
              seen[grp,al]++
              seenlist[grp]=al seenlist[grp]
              nseen[grp]++
            }
          }
        }
        if ( args["het"] == 1 && nalleles[grp] ) {
          nUsed[grp]++
          if ( args["mult"] == 1 || nseen[grp]<=2 ) {
            for ( x in thisal ) { numerator[grp]+=thisal[x]*(nalleles[grp]-thisal[x]) }
            denominator[grp]+=nalleles[grp]*(nalleles[grp]-1)
          }
          if ( args["persite"] == 1 ) { printOutput( $1"_"$2, 1, g, ".", 1, "het", numerator[grp], denominator[grp], nalleles[grp], miss[g] ) }
          delete thisal
        }
      }

      if ( args["het"] != 1 ) {
        for ( g in seenlist ) {
          if ( miss[g]/(miss[g]+nalleles[g]) <= args["miss"] && ( args["mult"] == 1 || nseen[g]<=2 ) ) {
    
            # Increment number of sites used for calculation
            nUsed[g]++
            
            # Extract allele counts of the group
            delete thesealleles
            delete bothalleles
            delete xx
            # split(seenlist[g], xx, "") crashes on macOS in forked children so rewritten the function
            for ( i=1;i<=length(seenlist[g]);i++ ) { xx[i]=substr(seenlist[g], i, 1) }
            for (al in xx) {
              bothalleles[xx[al]]++
              thesealleles[xx[al]] = alleles[g,xx[al]]
            }
    
            # Add to pi: probability that two randomly picked alleles differ
            # New formula below from https://pubmed.ncbi.nlm.nih.gov/36031871/
            thisnum[g]=nalleles[g]^2
            thisden[g]=nalleles[g]*(nalleles[g]-1)
            for ( x in thesealleles ) { thisnum[g]-=thesealleles[x]^2 }
            #
            # Possible improvement: check if ploidy is mixed once per group
            if ( TAJLIKE && nseen[g]==2 ) {
              for (i=1;i<nalleles[g];i++) { a1[g]+=1/i; a2[g]+=1/i^2 }
              segr[g]+=recalcS_expected( 1, nalleles[g], nalleles[g]+miss[g] )
              truesegr[g]++
            }

            if ( args["persite"] == 1 ) { 
              if ( thisden[g] && args["nopi"] != 1 ) { printOutput( $1"_"$2, 1, g, ".", 1, "pi", thisnum[g], thisden[g], nalleles[g], miss[g] ) }
            } else {
                numerator[g]+=thisnum[g]; denominator[g]+=thisden[g] 
                if ( VERBOSE ) {
                  allmiss[g]+=miss[g]
                  allgeno[g]+=nalleles[g]
                }
              }
    
            # Calculate pi between this group and all other groups with <50% missing data
            if ( args["nodxy"] == 1 ) { continue }
            for ( g2 in seenlist ) {
              if ( g2 < g && miss[g2]/(miss[g2]+nalleles[g2]) <= args["miss"] && ( args["mult"] == 1 || nseen[g2]<=2 ) ) {
              
                # Is the union of allelic states from g1 and g2 bigger than nseen[g1]?
                poolsize=nseen[g]

                # Extract alleles of the group
                delete thosealleles
                #split(seenlist[g2], yy) replaced to make it work on macOS
                for ( i=1;i<=length(seenlist[g2]);i++ ) { yy[i]=substr(seenlist[g2], i, 1) }
                for (al in yy) {
                  bothalleles[yy[al]]++ # so far keeps alleles from comparisons of g with previous groups
                  thosealleles[yy[al]] = alleles[g2,yy[al]]
                  if ( !(yy[al] in thesealleles) ) { poolsize++ }
                }

                # If not args["mult"] == 1, proceed only if common allele pool has <=2 alleles
                if ( args["mult"] == 1 || poolsize <= 2 ) {

                  # Increment number of sites used for dxy
                  nUsed[g,g2]++
                  
                  # Add to dxy: probability that two alleles picked from two groups differ
                  # subtraction rather than addition inspired by https://pubmed.ncbi.nlm.nih.gov/36031871/
                  thisnum[g,g2]=nalleles[g]*nalleles[g2]
                  thisden[g,g2]=thisnum[g,g2]
                  if ( FST ) { thisfstnum=""; thisfstden="" }
                  for ( x in bothalleles ) { 
                    # gawk breaks here if you don't set uninitialized array elems to 0
                    if ( !(x in thesealleles) ) { thesealleles[x]=0 }
                    else if ( !(x in thosealleles) ) { thosealleles[x]=0 }
                    thisnum[g,g2]-=thesealleles[x]*thosealleles[x]
                    if ( FST ) {
                      if ( FST=="HUD" ) { 
                        incrementFstHudson( thesealleles[x], thosealleles[x], nalleles[g], nalleles[g2] )
                      } else if ( FST=="WC" ) { 
                        incrementFstWeirCockerham( thesealleles[x], thosealleles[x], nalleles[g], nalleles[g2] )
                      } 
                      if ( args["mult"] != 1 ) { break } # no need to increment twice for biallelic comparisons
                    }
                  }
                  if ( args["rho"] == 1 ) {
                    if (!thisden[g2]) {
                      thisnum[g2]=nalleles[g2]^2
                      thisden[g2]=nalleles[g2]*(nalleles[g2]-1)
                      for ( x in thosealleles ) { thisnum[g2]-=thesealleles[x2]^2 }
                    }
                    # Here Hs = average of pi values of two populations,
                    #      Ht = pi of two populations pooled,
                    #      Hsp = Hs corrected for ploidy
                    thisHtnum=(nalleles[g] + nalleles[g2])^2
                    thisHtden=(nalleles[g] + nalleles[g2]) * (nalleles[g] + nalleles[g2] - 1)
                    for ( x in bothalleles ) { thisHtnum-=(thesealleles[x]+thosealleles[x])^2 }
                    thisHsnum = ( (thisnum[g]*thisden[g2] ) + (thisnum[g2]*thisden[g]) )
                    thisHsden = 2 * thisden[g] * thisden[g2] # same as Hspden
                    thisHspnum = ( (thisnum[g]*thisden[g2]*(ploidy[g]-1)/ploidy[g] ) + (thisnum[g2]*thisden[g]*(ploidy[g2]-1)/ploidy[g2]) )
                  }
                  if ( args["persite"] == 1 ) { 
                    if ( thisden[g,g2] ) { printOutput( $1"_"$2, 1, g, g2, 1, "dxy", thisnum[g,g2], thisden[g,g2], nalleles[g]+nalleles[g2], miss[g]+miss[g2] ) }
                    if ( FST && thisfstden ) { printOutput( $1"_"$2, 1, g, g2, 1, "Fst_"FST, thisfstnum, thisfstden, nalleles[g]+nalleles[g2], miss[g]+miss[g2] ) }
                    if ( args["rho"] == 1 ) { 
                      Hs = thisHsnum/thisHsden
                      Ht = thisHtnum/thisHtden
                      Hsp = thisHspnum/thisHsden
                      Hpt = Hs + 2 * (Ht - Hs)
                      thisrhonum=Hpt-Hs
                      thisrhoden=Hpt-Hsp
                      printOutput( $1"_"$2, 1, g, g2, 1, "Rho", thisrhonum, thisrhoden, nalleles[g]+nalleles[g2], miss[g]+miss[g2] ) 
                    }
                  } else {
                      numerator[g,g2]+=thisnum[g,g2]
                      denominator[g,g2]+=thisden[g,g2] 
                      if ( FST ) { 
                        fst_numerator[g,g2]+=thisfstnum
                        fst_denominator[g,g2]+=thisfstden
                      }
                      if ( args["rho"] == 1 ) {
                        Htnum[g,g2] += thisHtnum 
                        Htden[g,g2] += thisHtden 
                        Hsnum[g,g2] += thisHsnum
                        Hsden[g,g2] += thisHsden 
                        Hspnum[g,g2] += thisHspnum
                      }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  close(cmd)
  if ( pid>0 ) { return 0 }
  if ( args["persite"] != 1 ) {
    
    if ( length(minpos)>0 ) { 
      for ( i in minpos ) {
        nSites+=maxpos[i]-minpos[i]+1
      }
    }

  	for (g in groups) {
      if ( nUsed[g] ) {
        pinum[g] = numerator[g]
        piden[g] = denominator[g] 
        if ( args["nopi"] != 1 ) {
          printOutput( locus, nSites, g, ".", nUsed[g], args["het"] == 1 ? "het" : "pi", pinum[g], piden[g], allgeno[g], allmiss[g] )
        }
        if ( TAJLIKE && segr[g] ) {
          tP[g]=pinum[g]/piden[g]*nUsed[g]
          a1[g]/=truesegr[g]
          a2[g]/=truesegr[g]
          if ( args["tajima"] ) { segr[g]=truesegr[g] }
          tajnum[g]= tP[g] - segr[g]/a1[g]
          tajden[g]=calcTajimaVar( segr[g], a1[g], a2[g], nalleles[g]+miss[g] )
          printOutput( locus, nSites, g, ".", nUsed[g], args["tajima"] ? "TajD" : "TajD-like", tajnum[g], tajden[g], allgeno[g], allmiss[g] )
        }
      }
    }
  	for (i in combs) {
      if ( nUsed[i] ) { 
        dxynum[i] = numerator[i]
        dxyden[i] = denominator[i] 
        #split(i, ii, SUBSEP)`replaced to make it work on macOS`
        ii[1]=substr(i, 1, match(i, SUBSEP)-1)
        ii[2]=substr(i, match(i, SUBSEP)+1)
        printOutput( locus, nSites, ii[1], ii[2], nUsed[i], "Dxy", dxynum[i], dxyden[i], allgeno[ii[1]]+allgeno[ii[2]], allmiss[ii[1]]+allmiss[ii[2]] )
        if ( fst_denominator[i] ) { 
          printOutput( locus, nSites, ii[1], ii[2], nUsed[i], "Fst_"FST, fst_numerator[i], fst_denominator[i], allgeno[ii[1]]+allgeno[ii[2]], allmiss[ii[1]]+allmiss[ii[2]] ) 
        }
        if ( args["rho"] == 1 && Hsden[i] && Htden[i] ) { 
          Hs = Hsnum[i]/Hsden[i]
          Ht = Htnum[i]/Htden[i]
          Hsp = Hspnum[i]/Hsden[i]
          Hpt = Hs + 2 * (Ht - Hs)
          rho_numerator=Hpt-Hs
          rho_denominator=Hpt-Hsp
          printOutput( locus, nSites, ii[1], ii[2], nUsed[i], "Rho", rho_numerator, rho_denominator, allgeno[ii[1]]+allgeno[ii[2]], allmiss[ii[1]]+allmiss[ii[2]] ) 
        }
      }
    }
  }
}

END {
  if (jobnum==1) { print "" > "/dev/stderr" } # to start next prompt on new line
  if (pid>0) {
    while(wait()>0){}
  }
}

### FUNCTIONS

function printOutput( locus, nSites, pop1, pop2, nUsed, metric, numerator, denominator, nGeno, nMiss ) {
  out=locus"\t"nSites"\t"pop1"\t"pop2"\t"nUsed"\t"metric"\t"numerator/denominator
  if ( VERBOSE ) { 
      out = out"\t"numerator"\t"denominator"\t"nGeno"\t"nMiss
  }
  if ( args["bed"] == "" && args["jobs"] > 1 && args["persite"] != 1 ) {
    print out | "summarize_blks.awk"
  } else {
    print out
  }
}

function incrementFstHudson( a1, a2, n1, n2 ) {
  pi1 = a1 * (n1 - a1) / (n1*(n1-1))
  pi2 = a2 * (n2 - a2) / (n2*(n2-1))
  hw = pi1 + pi2
  hb = ( a1 * (n2-a2) + a2*(n1-a1) ) / (n1*n2)
  thisfstnum+=hb-hw
  thisfstden+=hb
} 

function incrementFstWeirCockerham( a1, a2, n1, n2 ) {
  # Formula from Bhatia et al. 2013, eq. (6)
  sizes = n1 * n2 / ( n1 + n2 )
  frac = 1 / ( n1 + n2 - 2 )
  mism = n1 * ( a1 / n1 ) * ( 1 - a1 / n1 ) + n2 * ( a2 / n2 ) * ( 1 - a2 / n2 )

  den = sizes * ( a1 / n1 - a2 / n2 )^2 + ( 2 * sizes - 1 ) * frac * mism
  if ( ! den ) { return } # WC Fst is NaN at uniform sites
  thisfstnum += den - 2 * sizes * frac * mism
  thisfstden += den
}

# Tajima's D-like statistic calculation
# The formula below is classic Tajima's D but with missing data we calculate a1 and a2 in a slightly different way
function calcTajimaVar( S, a1, a2, n ) {
  b1=(n+1)/(3*n-3)
  b2=(2*(n^2+n+3))/(9*n*(n-1))
  c1=b1-1/a1
  c2=b2-(n+2)/(a1*n)+a2/(a1^2)
  e1=c1/a1
  e2=c2/(a1^2+a2)
  return sqrt(e1*S + e2*S*(S-1))
}

function recalcS_expected(S, n1, n2,    coef1, coef2 ) {
  for (i=1; i<n1; i++) { coef1+=(1/i+1/(n1-i)) }
  for (i=1; i<n2; i++) { coef2+=(1/i+1/(n2-i)) }
  return S*(coef2/coef1)
}

function assert(condition, explanation) {
  if (! condition) {
    print help
    printf("Error %s:%d: %s\n",
           FILENAME, FNR, explanation) > "/dev/stderr"
    exit 1
  }
}

function print_header() {
  if ( VERBOSE != 1 ) {
    print "locus", "nSites", "pop1", "pop2", "nUsed", "metric", "value"
  } else {
    print "locus", "nSites", "pop1", "pop2", "nUsed", "metric", "value", "numerator", "denominator", "nGeno", "nMiss"
  }
}

function check_file(file,   cmd) {
   if (stat(file, _) < 0) {
        print "Error: could not read file "file > "/dev/stderr"
        exit 1
    }
}
