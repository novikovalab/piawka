#!/bin/sh
"export" "AWKPATH=$( dirname $0 ):$AWKPATH"
"export" "LC_ALL=C"
"exec" "gawk" "-v" "PIAWKADIR=$( dirname $0 )" -f "$0" "--" "$@" && 0 {}

@load "fork"
@load "filefuncs"

@include "getopt.awk"
@include "argparse.awk"

BEGIN{ 
  check_gawk_version()
  OFS="\t"
  usage="piawka v0.8.9\nUsage:\npiawka -g groups_tsv -v vcf_gz [OPTIONS]"
  SIGNAL_END_OF_BUFFER=SUBSEP SUBSEP SUBSEP
  summarize_blks=PIAWKADIR"/summarize_blks.awk"
  make_windows=PIAWKADIR"/make_windows.awk"
  aggregate_regions=PIAWKADIR"/aggregate_regions.awk"
  exit main()
}

function check_gawk_version(    gawk_version) {
  gawk_version = substr(PROCINFO["version"],1,index(PROCINFO["version"],".")-1)
  if ( gawk_version < 5 ) {
    print "Error: GNU AWK v5.0.0 or above is needed to run piawka"
    exit 1
  }
}

function main() {
  # add_argument args: shortopt, longopt, is_flag, description
  argparse::add_argument("1", "persite", 1, "output values for each site")
  argparse::add_argument("a", "all", 1, "output more cols: numerator, denominator, nGeno, nMiss")
  argparse::add_argument("b", "bed", 0, "BED file with regions to be analyzed")
  argparse::add_argument("B", "targets", 0, "BED file with targets (faster for numerous small regions)")
  argparse::add_argument("D", "nodxy", 1, "do not output Dxy")
  argparse::add_argument("f", "fst", 1, "output Hudson Fst")
  argparse::add_argument("F", "fstwc", 1, "output Weir and Cockerham Fst instead")
  argparse::add_argument("g", "groups", 0, "either 2-columns sample / group table or \nkeywords \"unite\" (1 group) or \"divide\" (n_samples groups)")
  argparse::add_argument("h", "help", 1, "show this help message")
  argparse::add_argument("H", "het", 1, "output only per-sample pi = heterozygosity")
  argparse::add_argument("j", "jobs", 0, "number of parallel jobs to run")
  argparse::add_argument("m", "mult", 1, "use multiallelic sites")
  argparse::add_argument("M", "miss", 0, "max share of missing GT per group at site, 0.0-1.0")
  argparse::add_argument("P", "nopi", 1, "do not output pi")
  argparse::add_argument("q", "quiet", 1, "do not output progress and warning messages")
  argparse::add_argument("r", "rho", 1, "output Ronfort's rho")
  argparse::add_argument("t", "tajimalike", 1, "output TajimaD-like stat (manages missing data but untested)")
  argparse::add_argument("T", "tajima", 1, "output classic TajimaD instead (affected by missing data)")
  argparse::add_argument("v", "vcf", 0, "gzipped and tabixed VCF file")
  help = usage "\n" argparse::format_help()
  getopt::Optind = 1 # start with 1st opt
  getopt::Opterr = 1 # print getopt errs
  if ( argparse::parse_args() != 0 ) { print help; return 0 }
  for(i in argparse::args) { args[i]=argparse::args[i] } # shorten args array name
  if (args["help"] || ARGC==1 ) { print help; return 0 }

  assert( args["vcf"] != "", "required argument: -v <file.vcf.gz>" )
  assert( ( args["groups"] != "" || args["het"] == 1 ), "required argument: -g <groups.tsv>" )
  if ( args["bed"] != "" ) { check_file( args["bed"] ) }
  if ( args["targets"] != "" ) { check_file( args["targets"] ) }

  # Set default variable values
  if ( args["fst"] == 1 ) { FST="HUD" }
  if ( args["fstwc"] == 1 ) { FST="WC" }
  if ( args["miss"] == "" ) { args["miss"]=1 }
  if ( args["mult"] == 1 && args["rho"] == 1 ) {
    say("Warning: rho was not meant for multiallelic sites")
  }
    
  PAIRWISE=( !args["nodxy"] || args["fst"] || args["fstwc"] || args["rho"] )
  VERBOSE=( args["all"] || ( args["jobs"]>1 && args["bed"]=="" ) )
  TAJLIKE=( args["tajima"] || args["tajimalike"] )

  if ( ( args["groups"] != "unite" ) && ( args["groups"] != "divide" ) ) {
    get_groups()
  }
  get_header()
  print_header()

  if ( args["jobs"] > 1 ) {
    if ( args["bed"] == "" ) {
      printf "" | summarize_blks # initialize pipe
    }
  } else { 
    args["jobs"]=1 
  }

  # The lines below are only executed if many jobs are invoked

  for ( jobnum=0; jobnum < args["jobs"]; jobnum++ ) { 
    if ( args["jobs"]==1 && args["bed"]=="" && args["targets"]=="" ) {break}
    buffer = "cat #"jobnum
    buffers[buffer]++
    printf "" |& buffer # initialize pipe
    pid=fork()
    if ( pid>0 ) { continue } 
    close(buffer, "to") # children don't print to buffer
    while ( $0 != SIGNAL_END_OF_BUFFER ) {
      getl=(buffer |& getline)
      if (getl <= 0) {
        print "Error: job "jobnum" did not reach end of buffer:" > "/dev/stderr"
        print $0 > "/dev/stderr"
        return getl
      }
      NSITES=$3-$2
      reg = $1":"$2+1"-"$3
      name = ( $4 == "" ? reg : $4 )
      say("Job " jobnum " window/region " ++bufline, 1)
      process_sites(reg, name)
    }
    close(buffer)
    return 0
  }
  
  # The lines below are executed by the parent

  if ( args["jobs"]>1 || args["bed"] != "" || args["targets"] != "" ) {
    bedcmd = get_bedcmd()
    while ( bedcmd | getline > 0 ) {
      jobnum=bedline++ % args["jobs"]
      buffer="cat #"jobnum # number of pipes with regions == # jobs
      print $0 |& buffer
    }
    for (i in buffers) {
      print SIGNAL_END_OF_BUFFER |& i
      close(i, "to")
    }
    close( bedcmd )
  } else {
    process_sites("", args["vcf"])
    return 0
  } 
  return 0
}


function get_bedcmd() {
  if ( args["bed"] != "" ) {
    return "gawk 1 "args["bed"]
  } else if (args["targets"] != "") {
    return aggregate_regions" "args["targets"] 
  } else {
    return make_windows" -v "args["vcf"]" -s "header_length
  }
}

# Groups file (first in command line): store lists of group members in `groups` array
function get_groups() { 
  check_file( args["groups"] )
  while ( getline < args["groups"] > 0 ) {
    if (!checked_groups) { assert(NF==2, "the groups file must contain two columns"); checked_groups=1 }
      groupmem[$1]= ( args["het"] == 1 ? $1 : $2 )
  }
  close( args["groups"] )
}

# VCF header: assign samples to groups
function get_header() {
  cmd="bgzip -dc " args["vcf"]
  while ( cmd | getline > 0 ) { 
    header_length++
    if ($0 ~ /^#CHROM/ ) {

      # Assign sample positions to groups
      for (i=10; i<=NF; i++) {
        if ( args["groups"]=="unite" ) {
          groupindex[i]="all_samples"
          groups["all_samples"]++
        } else if ( args["groups"]=="divide" ) {
          groupindex[i]=$i
          groups[$i]++
        } else if ( groupmem[$i] != "" ) { 
          groupindex[i]=groupmem[$i]
          groups[groupmem[$i]]++
        }
      }
      # Store group combinations
      if ( PAIRWISE && args["persite"] != 1 ) {
        for (g in groups) {
          for (g2 in groups) {
            if (g2 < g) { combs[g,g2]++ }
          }
        }
      }
      # Assign ploidy to groups using variant calls from next line, detect mixed-ploidy groups
      if ( TAJLIKE || args["rho"] == 1 ) {
        cmd | getline
        # Count alleles for each group, then divide by number of samples
        for (i in groupindex) {
          ploidy[groupindex[i]] += ( index($i, ":")>0 ? index($i, ":") : length($i)+1 )
        }
        for (g in groups) {
          ploidy[g] /= 2*groups[g]
        }
        if ( ploidy[g] % 1 != 0 && ( TAJLIKE || args["rho"] == 1 ) ) {
          say("Warning: mixed ploidy population "g" will give unreliable results with " TAJLIKE ? "Tajima's D" : "Ronfort's rho")
        }
      }
      break
    }
  }
  close(cmd)
}

# Process VCF lines
function process_sites(region, locus,   
                       nUsed, nSites, minpos, maxpos, skip_header, vcfline,
                       numerator, denominator, 
                       allgeno, allmiss,
                       fst_numerator, fst_denominator,
                       segr, truesegr,
                       Hsnum, Hsden, Htnum, Htden, Hspnum) {
  if ( locus == "" ) { locus="_" } # empty locus breaks summarize_blks.awk
  # Skip header in case of bgzip stream
  if ( region != "" ) { 
    skip_header=header_length
    cmd="tabix "args["vcf"]( args["targets"]!=""?" -T "args["targets"]:"" )" "region 
  } else {
    cmd="bgzip -dc "args["vcf"]
}
  while ( ++skip_header <= header_length ) { cmd | getline }
  while ( cmd | getline > 0 ) {
    vcfline++
    if ( args["jobs"]==1 && args["persite"] != 1 && region == "" && vcfline % 10000 == 0 ) {
     say("Processed " vcfline " sites ", 1)
    }

    # Increment nSites
    if ( NSITES!="" ) { 
      nSites=NSITES
    } else if ( pid!="" ) { 
      nSites++ 
    } else {
      if ( !minpos[$1] ) { minpos[$1]=$2 }
      maxpos[$1]=$2
    }

    # Process only SNPs (possibly monomorphic or multiallelic)
    # To obtain results identical to ksamuk/pixy, set $5 !~ /\*|,|[ACGT][ACGT]/
    # To process SNPs that only have one ALT allele
    if ( $4 !~ /^[ACGT]$/ || $5 ~ /\*|[ACGT][ACGT]/ ) { continue }
    #TODO: add check for chromosome not to summarize stats across chrs
  
    if ( args["persite"] == 1 ) { $1=locus"_"$1 }
  
    # Reset site-specific parameters
    for (g in groups) { miss[g]=0; misind[g]=0; nalleles[g]=0; nseen[g]=0 }
    delete alleles
    delete seen
    delete seenlist
    delete thisnum
    delete thisden

    # Pool GT values for groups, count each state and missing data
    for (i in groupindex) {
      grp=groupindex[i]
      gtend=index( $i, ":" )
      if ( gtend==0 ) { gtend=length($i)+1 }
      for (c=1; c<gtend; c+=2) {
        al=substr($i,c,1)
        if ( al == "." ) {
          miss[grp]+=gtend/2
          break
        } else {
          if ( args["het"] == 1 ) { thisal[al]++ }
          alleles[grp,al]++
          nalleles[grp]++
          if ( !seen[grp,al] ) {
            seen[grp,al]++
            seenlist[grp]=al seenlist[grp]
            nseen[grp]++
          }
        }
      }
      if ( args["het"] == 1 && nalleles[grp] ) {
        nUsed[grp]++
        if ( args["mult"] == 1 || nseen[grp]<=2 ) {
          for ( x in thisal ) { numerator[grp]+=thisal[x]*(nalleles[grp]-thisal[x]) }
          denominator[grp]+=nalleles[grp]*(nalleles[grp]-1)
        }
        if ( args["persite"] == 1 ) { printOutput( $1"_"$2, 1, g, ".", 1, "het", numerator[grp], denominator[grp], nalleles[grp], miss[g] ) }
        delete thisal
      }
    }

    if ( args["het"] != 1 ) {
      for ( g in seenlist ) {
        if ( miss[g]/(miss[g]+nalleles[g]) > args["miss"] || ( args["mult"] != 1 && nseen[g]>2 ) ) { continue }

        # Increment number of sites used for calculation
        nUsed[g]++
        
        # Extract allele counts of the group
        delete thesealleles
        delete bothalleles
        for (i=1;i<=length(seenlist[g]);i++) {
          xx=substr(seenlist[g],i,1)
          bothalleles[xx]++
          thesealleles[xx] = alleles[g,xx]
        }

        # Add to pi: probability that two randomly picked alleles differ
        # New formula below from https://pubmed.ncbi.nlm.nih.gov/36031871/
        thisnum[g]=nalleles[g]^2
        thisden[g]=nalleles[g]*(nalleles[g]-1)
        for ( x in thesealleles ) { thisnum[g]-=thesealleles[x]^2 }

        if ( TAJLIKE && nseen[g]==2 ) {
          for (i=1;i<nalleles[g];i++) { a1[g]+=1/i; a2[g]+=1/i^2 }
          segr[g]+=recalcS_expected( 1, nalleles[g], nalleles[g]+miss[g] )
          truesegr[g]++
        }

        if ( args["persite"] == 1 ) { 
          if ( thisden[g] && args["nopi"] != 1 ) { printOutput( $1"_"$2, 1, g, ".", 1, "pi", thisnum[g], thisden[g], nalleles[g], miss[g] ) }
        } else {
            numerator[g]+=thisnum[g]; denominator[g]+=thisden[g] 
            if ( VERBOSE ) {
              allmiss[g]+=miss[g]
              allgeno[g]+=nalleles[g]
            }
          }

        # Calculate pi between this group and all other groups with <50% missing data
        if ( !PAIRWISE ) { continue }
        for ( g2 in seenlist ) {
          if ( g2 >= g || miss[g2]/(miss[g2]+nalleles[g2]) > args["miss"] || ( args["mult"] != 1 && nseen[g2]>2 ) ) { continue }
        
          # Is the union of allelic states from g1 and g2 bigger than nseen[g1]?
          poolsize=nseen[g]

          # Extract alleles of the group
          delete thosealleles
          for (i=1;i<=length(seenlist[g2]);i++) {
            yy=substr(seenlist[g2],i,1)
            bothalleles[yy]++ # so far keeps alleles from comparisons of g with previous groups
            thosealleles[yy] = alleles[g2,yy]
            if ( !(yy in thesealleles) ) { poolsize++ }
          }

          # If not args["mult"] == 1, proceed only if common allele pool has <=2 alleles
          if ( args["mult"] != 1 && poolsize > 2 ) { continue }

          # Increment number of sites used for dxy
          nUsed[g,g2]++
          
          # Add to dxy: probability that two alleles picked from two groups differ
          # subtraction rather than addition inspired by https://pubmed.ncbi.nlm.nih.gov/36031871/
          thisnum[g,g2]=nalleles[g]*nalleles[g2]
          thisden[g,g2]=thisnum[g,g2]
          if ( FST ) { thisfstnum=""; thisfstden="" }
          for ( x in bothalleles ) { 
            # gawk breaks here if you don't set uninitialized array elems to 0
            if ( !(x in thesealleles) ) { thesealleles[x]=0 }
            else if ( !(x in thosealleles) ) { thosealleles[x]=0 }
            thisnum[g,g2]-=thesealleles[x]*thosealleles[x]
            if ( FST ) {
              if ( FST=="HUD" ) { 
                incrementFstHudson( thesealleles[x], thosealleles[x], nalleles[g], nalleles[g2] )
              } else if ( FST=="WC" ) { 
                incrementFstWeirCockerham( thesealleles[x], thosealleles[x], nalleles[g], nalleles[g2] )
              } 
              if ( args["mult"] != 1 ) { break } # no need to increment twice for biallelic comparisons
            }
          }
          if ( args["rho"] == 1 ) {
            if (!thisden[g2]) {
              thisnum[g2]=nalleles[g2]^2
              thisden[g2]=nalleles[g2]*(nalleles[g2]-1)
              for ( x in thosealleles ) { thisnum[g2]-=thesealleles[x2]^2 }
            }
            # Here Hs = average of pi values of two populations,
            #      Ht = pi of two populations pooled,
            #      Hsp = Hs corrected for ploidy
            thisHtnum=(nalleles[g] + nalleles[g2])^2
            thisHtden=(nalleles[g] + nalleles[g2]) * (nalleles[g] + nalleles[g2] - 1)
            for ( x in bothalleles ) { thisHtnum-=(thesealleles[x]+thosealleles[x])^2 }
            thisHsnum = ( (thisnum[g]*thisden[g2] ) + (thisnum[g2]*thisden[g]) )
            thisHsden = 2 * thisden[g] * thisden[g2] # same as Hspden
            thisHspnum = ( (thisnum[g]*thisden[g2]*(ploidy[g]-1)/ploidy[g] ) + (thisnum[g2]*thisden[g]*(ploidy[g2]-1)/ploidy[g2]) )
          }
          if ( args["persite"] == 1 ) { 
            if ( thisden[g,g2] && args["nodxy"] != 1 ) { printOutput( $1"_"$2, 1, g, g2, 1, "Dxy", thisnum[g,g2], thisden[g,g2], nalleles[g]+nalleles[g2], miss[g]+miss[g2] ) }
            if ( FST && thisfstden ) { printOutput( $1"_"$2, 1, g, g2, 1, "Fst_"FST, thisfstnum, thisfstden, nalleles[g]+nalleles[g2], miss[g]+miss[g2] ) }
            if ( args["rho"] == 1 ) { 
              Hs = thisHsnum/thisHsden
              Ht = thisHtnum/thisHtden
              Hsp = thisHspnum/thisHsden
              Hpt = Hs + 2 * (Ht - Hs)
              thisrhonum=Hpt-Hs
              thisrhoden=Hpt-Hsp
              printOutput( $1"_"$2, 1, g, g2, 1, "Rho", thisrhonum, thisrhoden, nalleles[g]+nalleles[g2], miss[g]+miss[g2] ) 
            }
          } else {
              numerator[g,g2]+=thisnum[g,g2]
              denominator[g,g2]+=thisden[g,g2] 
              if ( FST ) { 
                fst_numerator[g,g2]+=thisfstnum
                fst_denominator[g,g2]+=thisfstden
              }
              if ( args["rho"] == 1 ) {
                Htnum[g,g2] += thisHtnum 
                Htden[g,g2] += thisHtden 
                Hsnum[g,g2] += thisHsnum
                Hsden[g,g2] += thisHsden 
                Hspnum[g,g2] += thisHspnum
              }
          }
        }
      }
    }
  }
  close(cmd)
  if ( pid>0 ) { return 0 }
  if ( args["persite"] == 1 ) { return 0 }
    
  if ( length(minpos)>0 ) { 
    for ( i in minpos ) {
      nSites+=maxpos[i]-minpos[i]+1
    }
  }

  for (g in groups) {
    if ( nUsed[g] <= 0 ) { continue }
    pinum[g] = numerator[g]
    piden[g] = denominator[g] 
    if ( args["nopi"] != 1 ) {
      printOutput( locus, nSites, g, ".", nUsed[g], args["het"] == 1 ? "het" : "pi", pinum[g], piden[g], allgeno[g], allmiss[g] )
    }
    if ( TAJLIKE && segr[g] ) {
      tP[g]=pinum[g]/piden[g]*nUsed[g]
      a1[g]/=truesegr[g]
      a2[g]/=truesegr[g]
      if ( args["tajima"] ) { segr[g]=truesegr[g] }
      tajnum[g]= tP[g] - segr[g]/a1[g]
      tajden[g]=calcTajimaVar( segr[g], a1[g], a2[g], nalleles[g]+miss[g] )
      printOutput( locus, nSites, g, ".", nUsed[g], args["tajima"] ? "TajD" : "TajD-like", tajnum[g], tajden[g], allgeno[g], allmiss[g] )
    }
  }
  for (i in combs) {
    if ( nUsed[i] <= 0 ) { continue } 
    dxynum[i] = numerator[i]
    dxyden[i] = denominator[i] 
    split(i, ii, SUBSEP)
    if ( args["nodxy"] != 1 ) {
    printOutput( locus, nSites, ii[1], ii[2], nUsed[i], "Dxy", dxynum[i], dxyden[i], allgeno[ii[1]]+allgeno[ii[2]], allmiss[ii[1]]+allmiss[ii[2]] )
    }
    if ( fst_denominator[i] ) { 
      printOutput( locus, nSites, ii[1], ii[2], nUsed[i], "Fst_"FST, fst_numerator[i], fst_denominator[i], allgeno[ii[1]]+allgeno[ii[2]], allmiss[ii[1]]+allmiss[ii[2]] ) 
    }
    if ( args["rho"] == 1 && Hsden[i] && Htden[i] ) { 
      Hs= Hsnum[i]/Hsden[i]
      Ht= Htnum[i]/Htden[i]
      Hsp= Hspnum[i]/Hsden[i]
      Hpt= Hs + 2 * (Ht - Hs)
      rho_numerator=Hpt-Hs
      rho_denominator=Hpt-Hsp
      printOutput( locus, nSites, ii[1], ii[2], nUsed[i], "Rho", rho_numerator, rho_denominator, allgeno[ii[1]]+allgeno[ii[2]], allmiss[ii[1]]+allmiss[ii[2]] ) 
    }
  }
}

END {
  if (pid>0) {
    close(summarize_blks)
    while(wait()>0){}
  }
  if (pid>0) { say("") } # to start next prompt on new line
}

function printOutput( locus, nSites, pop1, pop2, nUsed, metric, numerator, denominator, nGeno, nMiss ) {
  out=locus"\t"nSites"\t"pop1"\t"pop2"\t"nUsed"\t"metric"\t"numerator/denominator
  if ( VERBOSE ) { 
      out = out"\t"numerator"\t"denominator"\t"nGeno"\t"nMiss
  }
  say("", 1) # overwrite error messages
  if ( args["bed"] == "" && args["jobs"] > 1 && args["persite"] != 1 ) {
    print out | summarize_blks
  } else {
    print out
  }
}

function incrementFstHudson( a1, a2, n1, n2 ) {
  pi1 = a1 * (n1 - a1) / (n1*(n1-1))
  pi2 = a2 * (n2 - a2) / (n2*(n2-1))
  hw = pi1 + pi2
  hb = ( a1 * (n2-a2) + a2*(n1-a1) ) / (n1*n2)
  thisfstnum+=hb-hw
  thisfstden+=hb
} 

function incrementFstWeirCockerham( a1, a2, n1, n2 ) {
  # Formula from Bhatia et al. 2013, eq. (6)
  sizes = n1 * n2 / ( n1 + n2 )
  frac = 1 / ( n1 + n2 - 2 )
  mism = a1 * ( 1 - a1 / n1 ) + a2 * ( 1 - a2 / n2 )

  den = sizes * ( a1 / n1 - a2 / n2 )^2 + ( 2 * sizes - 1 ) * frac * mism
  if ( ! den ) { return } # WC Fst is NaN at uniform sites
  thisfstnum += den - 2 * sizes * frac * mism
  thisfstden += den
}

# Tajima's D-like statistic calculation
# The formula below is classic Tajima's D but with missing data we calculate a1 and a2 in a slightly different way
function calcTajimaVar( S, a1, a2, n ) {
  b1=(n+1)/(3*n-3)
  b2=(2*(n^2+n+3))/(9*n*(n-1))
  c1=b1-1/a1
  c2=b2-(n+2)/(a1*n)+a2/(a1^2)
  e1=c1/a1
  e2=c2/(a1^2+a2)
  return sqrt(e1*S + e2*S*(S-1))
}

function recalcS_expected(S, n1, n2,    coef1, coef2 ) {
  for (i=1; i<n1; i++) { coef1+=(1/i+1/(n1-i)) }
  for (i=1; i<n2; i++) { coef2+=(1/i+1/(n2-i)) }
  return S*(coef2/coef1)
}

function assert(condition, explanation) {
  if (! condition) {
    print help
    printf("Error %s:%d: %s\n",
           FILENAME, FNR, explanation) > "/dev/stderr"
    exit 1
  }
}

function print_header() {
  if ( VERBOSE != 1 ) {
    print "locus", "nSites", "pop1", "pop2", "nUsed", "metric", "value"
  } else {
    print "locus", "nSites", "pop1", "pop2", "nUsed", "metric", "value", "numerator", "denominator", "nGeno", "nMiss"
  }
}

function check_file(file,   cmd) {
   if (stat(file, _) < 0) {
        print "Error: could not read file "file > "/dev/stderr"
        exit 1
    }
}

function say(string, same_line) {
  if (args["quiet"]) { return }
  if (same_line) {
    printf "\033[2K\r" string > "/dev/stderr"
  } else {
    print string > "/dev/stderr"
  }
}
